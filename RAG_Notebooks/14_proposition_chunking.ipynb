{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_deployment = \"text-embedding-ada-002\"\n",
    "chat_deployment = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Proposition Chunking for Enhanced RAG\n",
    "\n",
    "In this notebook, I implement proposition chunking - an advanced technique to break down documents into atomic, factual statements for more accurate retrieval. Unlike traditional chunking that simply divides text by character count, proposition chunking preserves the semantic integrity of individual facts.\n",
    "\n",
    "Proposition chunking delivers more precise retrieval by:\n",
    "\n",
    "1. Breaking content into atomic, self-contained facts\n",
    "2. Creating smaller, more granular units for retrieval  \n",
    "3. Enabling more precise matching between queries and relevant content\n",
    "4. Filtering out low-quality or incomplete propositions\n",
    "\n",
    "Let's build a complete implementation without relying on LangChain or FAISS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "We begin by importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import fitz\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Text from a PDF File\n",
    "To implement RAG, we first need a source of textual data. In this case, we extract text from a PDF file using the PyMuPDF library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file and prints the first `num_chars` characters.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "    str: Extracted text from the PDF.\n",
    "    \"\"\"\n",
    "    # Open the PDF file\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # Initialize an empty string to store the extracted text\n",
    "\n",
    "    # Iterate through each page in the PDF\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # Get the page\n",
    "        text = page.get_text(\"text\")  # Extract text from the page\n",
    "        all_text += text  # Append the extracted text to the all_text string\n",
    "\n",
    "    return all_text  # Return the extracted text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking the Extracted Text\n",
    "Once we have the extracted text, we divide it into smaller, overlapping chunks to improve retrieval accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=800, overlap=100):\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to chunk\n",
    "        chunk_size (int): Size of each chunk in characters\n",
    "        overlap (int): Overlap between chunks in characters\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: List of chunk dictionaries with text and metadata\n",
    "    \"\"\"\n",
    "    chunks = []  # Initialize an empty list to store the chunks\n",
    "    \n",
    "    # Iterate over the text with the specified chunk size and overlap\n",
    "    for i in range(0, len(text), chunk_size - overlap):\n",
    "        chunk = text[i:i + chunk_size]  # Extract a chunk of the specified size\n",
    "        if chunk:  # Ensure we don't add empty chunks\n",
    "            chunks.append({\n",
    "                \"text\": chunk,  # The chunk text\n",
    "                \"chunk_id\": len(chunks) + 1,  # Unique ID for the chunk\n",
    "                \"start_char\": i,  # Starting character index of the chunk\n",
    "                \"end_char\": i + len(chunk)  # Ending character index of the chunk\n",
    "            })\n",
    "    \n",
    "    print(f\"Created {len(chunks)} text chunks\")  # Print the number of created chunks\n",
    "    return chunks  # Return the list of chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the OpenAI API Client\n",
    "We initialize the OpenAI client to generate embeddings and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Azure OpenAI client with the endpoint and API key\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_version = os.getenv(\"API_VERSION\")\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_version=api_version,\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Vector Store Implementation\n",
    "We'll create a basic vector store to manage document chunks and their embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    A simple vector store implementation using NumPy.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Initialize lists to store vectors, texts, and metadata\n",
    "        self.vectors = []\n",
    "        self.texts = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        Add an item to the vector store.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The text content\n",
    "            embedding (List[float]): The embedding vector\n",
    "            metadata (Dict, optional): Additional metadata\n",
    "        \"\"\"\n",
    "        # Append the embedding, text, and metadata to their respective lists\n",
    "        self.vectors.append(np.array(embedding))\n",
    "        self.texts.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "    \n",
    "    def add_items(self, texts, embeddings, metadata_list=None):\n",
    "        \"\"\"\n",
    "        Add multiple items to the vector store.\n",
    "        \n",
    "        Args:\n",
    "            texts (List[str]): List of text contents\n",
    "            embeddings (List[List[float]]): List of embedding vectors\n",
    "            metadata_list (List[Dict], optional): List of metadata dictionaries\n",
    "        \"\"\"\n",
    "        # If no metadata list is provided, create an empty dictionary for each text\n",
    "        if metadata_list is None:\n",
    "            metadata_list = [{} for _ in range(len(texts))]\n",
    "        \n",
    "        # Add each text, embedding, and metadata to the store\n",
    "        for text, embedding, metadata in zip(texts, embeddings, metadata_list):\n",
    "            self.add_item(text, embedding, metadata)\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        Find the most similar items to a query embedding.\n",
    "        \n",
    "        Args:\n",
    "            query_embedding (List[float]): Query embedding vector\n",
    "            k (int): Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: Top k most similar items\n",
    "        \"\"\"\n",
    "        # Return an empty list if there are no vectors in the store\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "        \n",
    "        # Convert query embedding to a numpy array\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # Calculate similarities using cosine similarity\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))\n",
    "        \n",
    "        # Sort by similarity in descending order\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Collect the top k results\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": float(score)  # Convert to float for JSON serialization\n",
    "            })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(texts):\n",
    "    \"\"\"\n",
    "    Create embeddings for the given texts using Azure OpenAI.\n",
    "    \n",
    "    Args:\n",
    "        texts (str or List[str]): Input text(s)\n",
    "        \n",
    "    Returns:\n",
    "        List[List[float]] or List[float]: Embedding vector(s)\n",
    "    \"\"\"\n",
    "    # Handle both string and list inputs\n",
    "    input_texts = texts if isinstance(texts, list) else [texts]\n",
    "    \n",
    "    # Process in batches if needed (OpenAI API limits)\n",
    "    batch_size = 100\n",
    "    all_embeddings = []\n",
    "    \n",
    "    # Iterate over the input texts in batches\n",
    "    for i in range(0, len(input_texts), batch_size):\n",
    "        batch = input_texts[i:i + batch_size]  # Get the current batch of texts\n",
    "        \n",
    "        # Create embeddings for the current batch using Azure OpenAI\n",
    "        response = client.embeddings.create(\n",
    "            model=embedding_deployment,\n",
    "            input=batch\n",
    "        )\n",
    "        \n",
    "        # Extract embeddings from the response\n",
    "        batch_embeddings = [item.embedding for item in response.data]\n",
    "        all_embeddings.extend(batch_embeddings)  # Add the batch embeddings to the list\n",
    "    \n",
    "    # If input was a single string, return just the first embedding\n",
    "    if isinstance(texts, str):\n",
    "        return all_embeddings[0]\n",
    "    \n",
    "    # Otherwise, return all embeddings\n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposition Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_propositions(chunk):\n",
    "    \"\"\"\n",
    "    Generate atomic, self-contained propositions from a text chunk.\n",
    "    \n",
    "    Args:\n",
    "        chunk (Dict): Text chunk with content and metadata\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: List of generated propositions\n",
    "    \"\"\"\n",
    "    # System prompt to instruct the AI on how to generate propositions\n",
    "    system_prompt = \"\"\"Please break down the following text into simple, self-contained propositions. \n",
    "    Ensure that each proposition meets the following criteria:\n",
    "\n",
    "    1. Express a Single Fact: Each proposition should state one specific fact or claim.\n",
    "    2. Be Understandable Without Context: The proposition should be self-contained, meaning it can be understood without needing additional context.\n",
    "    3. Use Full Names, Not Pronouns: Avoid pronouns or ambiguous references; use full entity names.\n",
    "    4. Include Relevant Dates/Qualifiers: If applicable, include necessary dates, times, and qualifiers to make the fact precise.\n",
    "    5. Contain One Subject-Predicate Relationship: Focus on a single subject and its corresponding action or attribute, without conjunctions or multiple clauses.\n",
    "\n",
    "    Output ONLY the list of propositions without any additional text or explanations.\"\"\"\n",
    "\n",
    "    # User prompt containing the text chunk to be converted into propositions\n",
    "    user_prompt = f\"Text to convert into propositions:\\n\\n{chunk['text']}\"\n",
    "    \n",
    "    # Generate response from the model using Azure OpenAI\n",
    "    response = client.chat.completions.create(\n",
    "        model=chat_deployment,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Extract propositions from the response\n",
    "    raw_propositions = response.choices[0].message.content.strip().split('\\n')\n",
    "    \n",
    "    # Clean up propositions (remove numbering, bullets, etc.)\n",
    "    clean_propositions = []\n",
    "    for prop in raw_propositions:\n",
    "        # Remove numbering (1., 2., etc.) and bullet points\n",
    "        cleaned = re.sub(r'^\\s*(\\d+\\.|\\-|\\*)\\s*', '', prop).strip()\n",
    "        if cleaned and len(cleaned) > 10:  # Simple filter for empty or very short propositions\n",
    "            clean_propositions.append(cleaned)\n",
    "    \n",
    "    return clean_propositions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Checking for Propositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_proposition(proposition, original_text):\n",
    "    \"\"\"\n",
    "    Evaluate a proposition's quality based on accuracy, clarity, completeness, and conciseness.\n",
    "    \n",
    "    Args:\n",
    "        proposition (str): The proposition to evaluate\n",
    "        original_text (str): The original text for comparison\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Scores for each evaluation dimension\n",
    "    \"\"\"\n",
    "    # System prompt to instruct the AI on how to evaluate the proposition\n",
    "    system_prompt = \"\"\"You are an expert at evaluating the quality of propositions extracted from text.\n",
    "    Rate the given proposition on the following criteria (scale 1-10):\n",
    "\n",
    "    - Accuracy: How well the proposition reflects information in the original text\n",
    "    - Clarity: How easy it is to understand the proposition without additional context\n",
    "    - Completeness: Whether the proposition includes necessary details (dates, qualifiers, etc.)\n",
    "    - Conciseness: Whether the proposition is concise without losing important information\n",
    "\n",
    "    The response must be in valid JSON format with numerical scores for each criterion:\n",
    "    {\"accuracy\": X, \"clarity\": X, \"completeness\": X, \"conciseness\": X}\n",
    "    \"\"\"\n",
    "\n",
    "    # User prompt containing the proposition and the original text\n",
    "    user_prompt = f\"\"\"Proposition: {proposition}\n",
    "\n",
    "    Original Text: {original_text}\n",
    "\n",
    "    Please provide your evaluation scores in JSON format.\"\"\"\n",
    "\n",
    "    # Generate response from the model using Azure OpenAI\n",
    "    response = client.chat.completions.create(\n",
    "        model=chat_deployment,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Parse the JSON response\n",
    "    try:\n",
    "        scores = json.loads(response.choices[0].message.content.strip())\n",
    "        return scores\n",
    "    except json.JSONDecodeError:\n",
    "        # Fallback if JSON parsing fails\n",
    "        return {\n",
    "            \"accuracy\": 5,\n",
    "            \"clarity\": 5,\n",
    "            \"completeness\": 5,\n",
    "            \"conciseness\": 5\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Proposition Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document_into_propositions(pdf_path, chunk_size=800, chunk_overlap=100, \n",
    "                                      quality_thresholds=None):\n",
    "    \"\"\"\n",
    "    Process a document into quality-checked propositions.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        chunk_size (int): Size of each chunk in characters\n",
    "        chunk_overlap (int): Overlap between chunks in characters\n",
    "        quality_thresholds (Dict): Threshold scores for proposition quality\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[List[Dict], List[Dict]]: Original chunks and proposition chunks\n",
    "    \"\"\"\n",
    "    # Set default quality thresholds if not provided\n",
    "    if quality_thresholds is None:\n",
    "        quality_thresholds = {\n",
    "            \"accuracy\": 7,\n",
    "            \"clarity\": 7,\n",
    "            \"completeness\": 7,\n",
    "            \"conciseness\": 7\n",
    "        }\n",
    "    \n",
    "    # Extract text from the PDF file\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Create chunks from the extracted text\n",
    "    chunks = chunk_text(text, chunk_size, chunk_overlap)\n",
    "    \n",
    "    # Initialize a list to store all propositions\n",
    "    all_propositions = []\n",
    "    \n",
    "    print(\"Generating propositions from chunks...\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Processing chunk {i+1}/{len(chunks)}...\")\n",
    "        \n",
    "        # Generate propositions for the current chunk\n",
    "        chunk_propositions = generate_propositions(chunk)\n",
    "        print(f\"Generated {len(chunk_propositions)} propositions\")\n",
    "        \n",
    "        # Process each generated proposition\n",
    "        for prop in chunk_propositions:\n",
    "            proposition_data = {\n",
    "                \"text\": prop,\n",
    "                \"source_chunk_id\": chunk[\"chunk_id\"],\n",
    "                \"source_text\": chunk[\"text\"]\n",
    "            }\n",
    "            all_propositions.append(proposition_data)\n",
    "    \n",
    "    # Evaluate the quality of the generated propositions\n",
    "    print(\"\\nEvaluating proposition quality...\")\n",
    "    quality_propositions = []\n",
    "    \n",
    "    for i, prop in enumerate(all_propositions):\n",
    "        if i % 10 == 0:  # Status update every 10 propositions\n",
    "            print(f\"Evaluating proposition {i+1}/{len(all_propositions)}...\")\n",
    "            \n",
    "        # Evaluate the quality of the current proposition\n",
    "        scores = evaluate_proposition(prop[\"text\"], prop[\"source_text\"])\n",
    "        prop[\"quality_scores\"] = scores\n",
    "        \n",
    "        # Check if the proposition passes the quality thresholds\n",
    "        passes_quality = True\n",
    "        for metric, threshold in quality_thresholds.items():\n",
    "            if scores.get(metric, 0) < threshold:\n",
    "                passes_quality = False\n",
    "                break\n",
    "        \n",
    "        if passes_quality:\n",
    "            quality_propositions.append(prop)\n",
    "        else:\n",
    "            print(f\"Proposition failed quality check: {prop['text'][:50]}...\")\n",
    "    \n",
    "    print(f\"\\nRetained {len(quality_propositions)}/{len(all_propositions)} propositions after quality filtering\")\n",
    "    \n",
    "    return chunks, quality_propositions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Vector Stores for Both Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vector_stores(chunks, propositions):\n",
    "    \"\"\"\n",
    "    Build vector stores for both chunk-based and proposition-based approaches.\n",
    "    \n",
    "    Args:\n",
    "        chunks (List[Dict]): Original document chunks\n",
    "        propositions (List[Dict]): Quality-filtered propositions\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[SimpleVectorStore, SimpleVectorStore]: Chunk and proposition vector stores\n",
    "    \"\"\"\n",
    "    # Create vector store for chunks\n",
    "    chunk_store = SimpleVectorStore()\n",
    "    \n",
    "    # Extract chunk texts and create embeddings\n",
    "    chunk_texts = [chunk[\"text\"] for chunk in chunks]\n",
    "    print(f\"Creating embeddings for {len(chunk_texts)} chunks...\")\n",
    "    chunk_embeddings = create_embeddings(chunk_texts)\n",
    "    \n",
    "    # Add chunks to vector store with metadata\n",
    "    chunk_metadata = [{\"chunk_id\": chunk[\"chunk_id\"], \"type\": \"chunk\"} for chunk in chunks]\n",
    "    chunk_store.add_items(chunk_texts, chunk_embeddings, chunk_metadata)\n",
    "    \n",
    "    # Create vector store for propositions\n",
    "    prop_store = SimpleVectorStore()\n",
    "    \n",
    "    # Extract proposition texts and create embeddings\n",
    "    prop_texts = [prop[\"text\"] for prop in propositions]\n",
    "    print(f\"Creating embeddings for {len(prop_texts)} propositions...\")\n",
    "    prop_embeddings = create_embeddings(prop_texts)\n",
    "    \n",
    "    # Add propositions to vector store with metadata\n",
    "    prop_metadata = [\n",
    "        {\n",
    "            \"type\": \"proposition\", \n",
    "            \"source_chunk_id\": prop[\"source_chunk_id\"],\n",
    "            \"quality_scores\": prop[\"quality_scores\"]\n",
    "        } \n",
    "        for prop in propositions\n",
    "    ]\n",
    "    prop_store.add_items(prop_texts, prop_embeddings, prop_metadata)\n",
    "    \n",
    "    return chunk_store, prop_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query and Retrieval Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_from_store(query, vector_store, k=5):\n",
    "    \"\"\"\n",
    "    Retrieve relevant items from a vector store based on query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_store (SimpleVectorStore): Vector store to search\n",
    "        k (int): Number of results to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Retrieved items with scores and metadata\n",
    "    \"\"\"\n",
    "    # Create query embedding\n",
    "    query_embedding = create_embeddings(query)\n",
    "    \n",
    "    # Search vector store for the top k most similar items\n",
    "    results = vector_store.similarity_search(query_embedding, k=k)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_retrieval_approaches(query, chunk_store, prop_store, k=5):\n",
    "    \"\"\"\n",
    "    Compare chunk-based and proposition-based retrieval for a query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        chunk_store (SimpleVectorStore): Chunk-based vector store\n",
    "        prop_store (SimpleVectorStore): Proposition-based vector store\n",
    "        k (int): Number of results to retrieve from each store\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Comparison results\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Query: {query} ===\")\n",
    "    \n",
    "    # Retrieve results from the proposition-based vector store\n",
    "    print(\"\\nRetrieving with proposition-based approach...\")\n",
    "    prop_results = retrieve_from_store(query, prop_store, k)\n",
    "    \n",
    "    # Retrieve results from the chunk-based vector store\n",
    "    print(\"Retrieving with chunk-based approach...\")\n",
    "    chunk_results = retrieve_from_store(query, chunk_store, k)\n",
    "    \n",
    "    # Display proposition-based results\n",
    "    print(\"\\n=== Proposition-Based Results ===\")\n",
    "    for i, result in enumerate(prop_results):\n",
    "        print(f\"{i+1}) {result['text']} (Score: {result['similarity']:.4f})\")\n",
    "    \n",
    "    # Display chunk-based results\n",
    "    print(\"\\n=== Chunk-Based Results ===\")\n",
    "    for i, result in enumerate(chunk_results):\n",
    "        # Truncate text to keep the output manageable\n",
    "        truncated_text = result['text'][:150] + \"...\" if len(result['text']) > 150 else result['text']\n",
    "        print(f\"{i+1}) {truncated_text} (Score: {result['similarity']:.4f})\")\n",
    "    \n",
    "    # Return the comparison results\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"proposition_results\": prop_results,\n",
    "        \"chunk_results\": chunk_results\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Generation and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, results, result_type=\"proposition\"):\n",
    "    \"\"\"\n",
    "    Generate a response based on retrieved results.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        results (List[Dict]): Retrieved items\n",
    "        result_type (str): Type of results ('proposition' or 'chunk')\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    # Combine retrieved texts into a single context string\n",
    "    context = \"\\n\\n\".join([result[\"text\"] for result in results])\n",
    "    \n",
    "    # System prompt to instruct the AI on how to generate the response\n",
    "    system_prompt = f\"\"\"You are an AI assistant answering questions based on retrieved information.\n",
    "Your answer should be based on the following {result_type}s that were retrieved from a knowledge base.\n",
    "If the retrieved information doesn't answer the question, acknowledge this limitation.\"\"\"\n",
    "\n",
    "    # User prompt containing the query and the retrieved context\n",
    "    user_prompt = f\"\"\"Query: {query}\n",
    "\n",
    "Retrieved {result_type}s:\n",
    "{context}\n",
    "\n",
    "Please answer the query based on the retrieved information.\"\"\"\n",
    "\n",
    "    # Generate the response using Azure OpenAI\n",
    "    response = client.chat.completions.create(\n",
    "        model=chat_deployment,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    \n",
    "    # Return the generated response text\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_responses(query, prop_response, chunk_response, reference_answer=None):\n",
    "    \"\"\"\n",
    "    Evaluate and compare responses from both approaches.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        prop_response (str): Response from proposition-based approach\n",
    "        chunk_response (str): Response from chunk-based approach\n",
    "        reference_answer (str, optional): Reference answer for comparison\n",
    "        \n",
    "    Returns:\n",
    "        str: Evaluation analysis\n",
    "    \"\"\"\n",
    "    # System prompt to instruct the AI on how to evaluate the responses\n",
    "    system_prompt = \"\"\"You are an expert evaluator of information retrieval systems. \n",
    "    Compare the two responses to the same query, one generated from proposition-based retrieval \n",
    "    and the other from chunk-based retrieval.\n",
    "\n",
    "    Evaluate them based on:\n",
    "    1. Accuracy: Which response provides more factually correct information?\n",
    "    2. Relevance: Which response better addresses the specific query?\n",
    "    3. Conciseness: Which response is more concise while maintaining completeness?\n",
    "    4. Clarity: Which response is easier to understand?\n",
    "\n",
    "    Be specific about the strengths and weaknesses of each approach.\"\"\"\n",
    "\n",
    "    # User prompt containing the query and the responses to be compared\n",
    "    user_prompt = f\"\"\"Query: {query}\n",
    "\n",
    "    Response from Proposition-Based Retrieval:\n",
    "    {prop_response}\n",
    "\n",
    "    Response from Chunk-Based Retrieval:\n",
    "    {chunk_response}\"\"\"\n",
    "\n",
    "    # If a reference answer is provided, include it in the user prompt for factual checking\n",
    "    if reference_answer:\n",
    "        user_prompt += f\"\"\"\n",
    "\n",
    "    Reference Answer (for factual checking):\n",
    "    {reference_answer}\"\"\"\n",
    "\n",
    "    # Add the final instruction to the user prompt\n",
    "    user_prompt += \"\"\"\n",
    "    Please provide a detailed comparison of these two responses, highlighting which approach performed better and why.\"\"\"\n",
    "\n",
    "    # Generate the evaluation analysis using Azure OpenAI\n",
    "    response = client.chat.completions.create(\n",
    "        model=chat_deployment,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Return the generated evaluation analysis\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete End-to-End Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_proposition_chunking_evaluation(pdf_path, test_queries, reference_answers=None):\n",
    "    \"\"\"\n",
    "    Run a complete evaluation of proposition chunking vs standard chunking.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        test_queries (List[str]): List of test queries\n",
    "        reference_answers (List[str], optional): Reference answers for queries\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Evaluation results\n",
    "    \"\"\"\n",
    "    print(\"=== Starting Proposition Chunking Evaluation ===\\n\")\n",
    "    \n",
    "    # Process document into propositions and chunks\n",
    "    chunks, propositions = process_document_into_propositions(pdf_path)\n",
    "    \n",
    "    # Build vector stores for chunks and propositions\n",
    "    chunk_store, prop_store = build_vector_stores(chunks, propositions)\n",
    "    \n",
    "    # Initialize a list to store results for each query\n",
    "    results = []\n",
    "    \n",
    "    # Run tests for each query\n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"\\n\\n=== Testing Query {i+1}/{len(test_queries)} ===\")\n",
    "        print(f\"Query: {query}\")\n",
    "        \n",
    "        # Get retrieval results from both chunk-based and proposition-based approaches\n",
    "        retrieval_results = compare_retrieval_approaches(query, chunk_store, prop_store)\n",
    "        \n",
    "        # Generate responses based on the retrieved proposition-based results\n",
    "        print(\"\\nGenerating response from proposition-based results...\")\n",
    "        prop_response = generate_response(\n",
    "            query, \n",
    "            retrieval_results[\"proposition_results\"], \n",
    "            \"proposition\"\n",
    "        )\n",
    "        \n",
    "        # Generate responses based on the retrieved chunk-based results\n",
    "        print(\"Generating response from chunk-based results...\")\n",
    "        chunk_response = generate_response(\n",
    "            query, \n",
    "            retrieval_results[\"chunk_results\"], \n",
    "            \"chunk\"\n",
    "        )\n",
    "        \n",
    "        # Get reference answer if available\n",
    "        reference = None\n",
    "        if reference_answers and i < len(reference_answers):\n",
    "            reference = reference_answers[i]\n",
    "        \n",
    "        # Evaluate the generated responses\n",
    "        print(\"\\nEvaluating responses...\")\n",
    "        evaluation = evaluate_responses(query, prop_response, chunk_response, reference)\n",
    "        \n",
    "        # Compile results for the current query\n",
    "        query_result = {\n",
    "            \"query\": query,\n",
    "            \"proposition_results\": retrieval_results[\"proposition_results\"],\n",
    "            \"chunk_results\": retrieval_results[\"chunk_results\"],\n",
    "            \"proposition_response\": prop_response,\n",
    "            \"chunk_response\": chunk_response,\n",
    "            \"reference_answer\": reference,\n",
    "            \"evaluation\": evaluation\n",
    "        }\n",
    "        \n",
    "        # Append the results to the overall results list\n",
    "        results.append(query_result)\n",
    "        \n",
    "        # Print the responses and evaluation for the current query\n",
    "        print(\"\\n=== Proposition-Based Response ===\")\n",
    "        print(prop_response)\n",
    "        \n",
    "        print(\"\\n=== Chunk-Based Response ===\")\n",
    "        print(chunk_response)\n",
    "        \n",
    "        print(\"\\n=== Evaluation ===\")\n",
    "        print(evaluation)\n",
    "    \n",
    "    # Generate overall analysis of the evaluation\n",
    "    print(\"\\n\\n=== Generating Overall Analysis ===\")\n",
    "    overall_analysis = generate_overall_analysis(results)\n",
    "    print(\"\\n\" + overall_analysis)\n",
    "    \n",
    "    # Return the evaluation results, overall analysis, and counts of propositions and chunks\n",
    "    return {\n",
    "        \"results\": results,\n",
    "        \"overall_analysis\": overall_analysis,\n",
    "        \"proposition_count\": len(propositions),\n",
    "        \"chunk_count\": len(chunks)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_overall_analysis(results):\n",
    "    \"\"\"\n",
    "    Generate an overall analysis of proposition vs chunk approaches.\n",
    "    \n",
    "    Args:\n",
    "        results (List[Dict]): Results from each test query\n",
    "        \n",
    "    Returns:\n",
    "        str: Overall analysis\n",
    "    \"\"\"\n",
    "    # System prompt to instruct the AI on how to generate the overall analysis\n",
    "    system_prompt = \"\"\"You are an expert at evaluating information retrieval systems.\n",
    "    Based on multiple test queries, provide an overall analysis comparing proposition-based retrieval \n",
    "    to chunk-based retrieval for RAG (Retrieval-Augmented Generation) systems.\n",
    "\n",
    "    Focus on:\n",
    "    1. When proposition-based retrieval performs better\n",
    "    2. When chunk-based retrieval performs better\n",
    "    3. The overall strengths and weaknesses of each approach\n",
    "    4. Recommendations for when to use each approach\"\"\"\n",
    "\n",
    "    # Create a summary of evaluations for each query\n",
    "    evaluations_summary = \"\"\n",
    "    for i, result in enumerate(results):\n",
    "        evaluations_summary += f\"Query {i+1}: {result['query']}\\n\"\n",
    "        evaluations_summary += f\"Evaluation Summary: {result['evaluation'][:200]}...\\n\\n\"\n",
    "\n",
    "    # User prompt containing the summary of evaluations\n",
    "    user_prompt = f\"\"\"Based on the following evaluations of proposition-based vs chunk-based retrieval across {len(results)} queries, \n",
    "    provide an overall analysis comparing these two approaches:\n",
    "\n",
    "    {evaluations_summary}\n",
    "\n",
    "    Please provide a comprehensive analysis on the relative strengths and weaknesses of proposition-based \n",
    "    and chunk-based retrieval for RAG systems.\"\"\"\n",
    "\n",
    "    # Generate the overall analysis using Azure OpenAI\n",
    "    response = client.chat.completions.create(\n",
    "        model=chat_deployment,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Return the generated analysis text\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Proposition Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Proposition Chunking Evaluation ===\n",
      "\n",
      "Created 48 text chunks\n",
      "Generating propositions from chunks...\n",
      "Processing chunk 1/48...\n",
      "Generated 13 propositions\n",
      "Processing chunk 2/48...\n",
      "Generated 13 propositions\n",
      "Processing chunk 2/48...\n",
      "Generated 20 propositions\n",
      "Processing chunk 3/48...\n",
      "Generated 20 propositions\n",
      "Processing chunk 3/48...\n",
      "Generated 15 propositions\n",
      "Processing chunk 4/48...\n",
      "Generated 15 propositions\n",
      "Processing chunk 4/48...\n",
      "Generated 15 propositions\n",
      "Processing chunk 5/48...\n",
      "Generated 15 propositions\n",
      "Processing chunk 5/48...\n",
      "Generated 20 propositions\n",
      "Processing chunk 6/48...\n",
      "Generated 20 propositions\n",
      "Processing chunk 6/48...\n",
      "Generated 19 propositions\n",
      "Processing chunk 7/48...\n",
      "Generated 19 propositions\n",
      "Processing chunk 7/48...\n",
      "Generated 21 propositions\n",
      "Processing chunk 8/48...\n",
      "Generated 21 propositions\n",
      "Processing chunk 8/48...\n",
      "Generated 21 propositions\n",
      "Processing chunk 9/48...\n",
      "Generated 21 propositions\n",
      "Processing chunk 9/48...\n",
      "Generated 23 propositions\n",
      "Processing chunk 10/48...\n",
      "Generated 23 propositions\n",
      "Processing chunk 10/48...\n",
      "Generated 22 propositions\n",
      "Processing chunk 11/48...\n",
      "Generated 22 propositions\n",
      "Processing chunk 11/48...\n",
      "Generated 16 propositions\n",
      "Processing chunk 12/48...\n",
      "Generated 16 propositions\n",
      "Processing chunk 12/48...\n",
      "Generated 17 propositions\n",
      "Processing chunk 13/48...\n",
      "Generated 17 propositions\n",
      "Processing chunk 13/48...\n",
      "Generated 13 propositions\n",
      "Processing chunk 14/48...\n",
      "Generated 13 propositions\n",
      "Processing chunk 14/48...\n",
      "Generated 15 propositions\n",
      "Processing chunk 15/48...\n",
      "Generated 15 propositions\n",
      "Processing chunk 15/48...\n",
      "Generated 18 propositions\n",
      "Processing chunk 16/48...\n",
      "Generated 18 propositions\n",
      "Processing chunk 16/48...\n",
      "Generated 14 propositions\n",
      "Processing chunk 17/48...\n",
      "Generated 14 propositions\n",
      "Processing chunk 17/48...\n",
      "Generated 22 propositions\n",
      "Processing chunk 18/48...\n",
      "Generated 22 propositions\n",
      "Processing chunk 18/48...\n",
      "Generated 9 propositions\n",
      "Processing chunk 19/48...\n",
      "Generated 9 propositions\n",
      "Processing chunk 19/48...\n",
      "Generated 18 propositions\n",
      "Processing chunk 20/48...\n",
      "Generated 18 propositions\n",
      "Processing chunk 20/48...\n",
      "Generated 23 propositions\n",
      "Processing chunk 21/48...\n",
      "Generated 23 propositions\n",
      "Processing chunk 21/48...\n",
      "Generated 15 propositions\n",
      "Processing chunk 22/48...\n",
      "Generated 15 propositions\n",
      "Processing chunk 22/48...\n",
      "Generated 18 propositions\n",
      "Processing chunk 23/48...\n",
      "Generated 18 propositions\n",
      "Processing chunk 23/48...\n",
      "Generated 24 propositions\n",
      "Processing chunk 24/48...\n",
      "Generated 24 propositions\n",
      "Processing chunk 24/48...\n",
      "Generated 20 propositions\n",
      "Processing chunk 25/48...\n",
      "Generated 20 propositions\n",
      "Processing chunk 25/48...\n",
      "Generated 17 propositions\n",
      "Processing chunk 26/48...\n",
      "Generated 17 propositions\n",
      "Processing chunk 26/48...\n",
      "Generated 20 propositions\n",
      "Processing chunk 27/48...\n",
      "Generated 20 propositions\n",
      "Processing chunk 27/48...\n",
      "Generated 15 propositions\n",
      "Processing chunk 28/48...\n",
      "Generated 15 propositions\n",
      "Processing chunk 28/48...\n",
      "Generated 19 propositions\n",
      "Processing chunk 29/48...\n",
      "Generated 19 propositions\n",
      "Processing chunk 29/48...\n",
      "Generated 20 propositions\n",
      "Processing chunk 30/48...\n",
      "Generated 20 propositions\n",
      "Processing chunk 30/48...\n",
      "Generated 16 propositions\n",
      "Processing chunk 31/48...\n",
      "Generated 16 propositions\n",
      "Processing chunk 31/48...\n",
      "Generated 21 propositions\n",
      "Processing chunk 32/48...\n",
      "Generated 21 propositions\n",
      "Processing chunk 32/48...\n",
      "Generated 20 propositions\n",
      "Processing chunk 33/48...\n",
      "Generated 20 propositions\n",
      "Processing chunk 33/48...\n",
      "Generated 18 propositions\n",
      "Processing chunk 34/48...\n",
      "Generated 18 propositions\n",
      "Processing chunk 34/48...\n",
      "Generated 17 propositions\n",
      "Processing chunk 35/48...\n",
      "Generated 17 propositions\n",
      "Processing chunk 35/48...\n",
      "Generated 16 propositions\n",
      "Processing chunk 36/48...\n",
      "Generated 16 propositions\n",
      "Processing chunk 36/48...\n",
      "Generated 18 propositions\n",
      "Processing chunk 37/48...\n",
      "Generated 18 propositions\n",
      "Processing chunk 37/48...\n",
      "Generated 21 propositions\n",
      "Processing chunk 38/48...\n",
      "Generated 21 propositions\n",
      "Processing chunk 38/48...\n",
      "Generated 12 propositions\n",
      "Processing chunk 39/48...\n",
      "Generated 12 propositions\n",
      "Processing chunk 39/48...\n",
      "Generated 22 propositions\n",
      "Processing chunk 40/48...\n",
      "Generated 22 propositions\n",
      "Processing chunk 40/48...\n",
      "Generated 20 propositions\n",
      "Processing chunk 41/48...\n",
      "Generated 20 propositions\n",
      "Processing chunk 41/48...\n",
      "Generated 18 propositions\n",
      "Processing chunk 42/48...\n",
      "Generated 18 propositions\n",
      "Processing chunk 42/48...\n",
      "Generated 26 propositions\n",
      "Processing chunk 43/48...\n",
      "Generated 26 propositions\n",
      "Processing chunk 43/48...\n",
      "Generated 19 propositions\n",
      "Processing chunk 44/48...\n",
      "Generated 19 propositions\n",
      "Processing chunk 44/48...\n",
      "Generated 18 propositions\n",
      "Processing chunk 45/48...\n",
      "Generated 18 propositions\n",
      "Processing chunk 45/48...\n",
      "Generated 17 propositions\n",
      "Processing chunk 46/48...\n",
      "Generated 17 propositions\n",
      "Processing chunk 46/48...\n",
      "Generated 22 propositions\n",
      "Processing chunk 47/48...\n",
      "Generated 22 propositions\n",
      "Processing chunk 47/48...\n",
      "Generated 14 propositions\n",
      "Processing chunk 48/48...\n",
      "Generated 14 propositions\n",
      "Processing chunk 48/48...\n",
      "Generated 16 propositions\n",
      "\n",
      "Evaluating proposition quality...\n",
      "Evaluating proposition 1/873...\n",
      "Generated 16 propositions\n",
      "\n",
      "Evaluating proposition quality...\n",
      "Evaluating proposition 1/873...\n",
      "Evaluating proposition 11/873...\n",
      "Evaluating proposition 11/873...\n",
      "Proposition failed quality check: Artificial intelligence has been a topic of intere...\n",
      "Proposition failed quality check: Artificial intelligence has been a topic of intere...\n",
      "Proposition failed quality check: Artificial intelligence is often depicted in myths...\n",
      "Proposition failed quality check: Artificial intelligence is often depicted in myths...\n",
      "Evaluating proposition 21/873...\n",
      "Evaluating proposition 21/873...\n",
      "Proposition failed quality check: Artificial intelligence is impacting how people li...\n",
      "Proposition failed quality check: Artificial intelligence is impacting how people li...\n",
      "Proposition failed quality check: Artificial intelligence is impacting how people wo...\n",
      "Evaluating proposition 31/873...\n",
      "Proposition failed quality check: Artificial intelligence is impacting how people wo...\n",
      "Evaluating proposition 31/873...\n",
      "Proposition failed quality check: Artificial intelligence is impacting how people in...\n",
      "Proposition failed quality check: Artificial intelligence is impacting how people in...\n",
      "Proposition failed quality check: The development of self-driving cars involves arti...\n",
      "Proposition failed quality check: The development of self-driving cars involves arti...\n",
      "Proposition failed quality check: Advanced medical diagnostics involve artificial in...\n",
      "Proposition failed quality check: Advanced medical diagnostics involve artificial in...\n",
      "Evaluating proposition 41/873...\n",
      "Evaluating proposition 41/873...\n",
      "Proposition failed quality check: Supervised learning is a type of machine learning....\n",
      "Proposition failed quality check: Supervised learning is a type of machine learning....\n",
      "Evaluating proposition 51/873...\n",
      "Evaluating proposition 51/873...\n",
      "Evaluating proposition 61/873...\n",
      "Evaluating proposition 61/873...\n",
      "Proposition failed quality check: An agent operates in an environment to maximize a ...\n",
      "Proposition failed quality check: An agent operates in an environment to maximize a ...\n",
      "Evaluating proposition 71/873...\n",
      "Evaluating proposition 71/873...\n",
      "Evaluating proposition 81/873...\n",
      "Evaluating proposition 81/873...\n",
      "Evaluating proposition 91/873...\n",
      "Evaluating proposition 91/873...\n",
      "Proposition failed quality check: Natural Language Processing (NLP) focuses on enabl...\n",
      "Proposition failed quality check: Natural Language Processing (NLP) focuses on enabl...\n",
      "Evaluating proposition 101/873...\n",
      "Evaluating proposition 101/873...\n",
      "Proposition failed quality check: Sentiment analysis is an application of artificial...\n",
      "Proposition failed quality check: Sentiment analysis is an application of artificial...\n",
      "Evaluating proposition 111/873...\n",
      "Evaluating proposition 111/873...\n",
      "Evaluating proposition 121/873...\n",
      "Evaluating proposition 121/873...\n",
      "Evaluating proposition 131/873...\n",
      "Evaluating proposition 131/873...\n",
      "Evaluating proposition 141/873...\n",
      "Evaluating proposition 141/873...\n",
      "Proposition failed quality check: Customer service chatbots are used in AI-powered s...\n",
      "Proposition failed quality check: Customer service chatbots are used in AI-powered s...\n",
      "Proposition failed quality check: AI-powered systems are used for supply chain optim...\n",
      "Proposition failed quality check: AI-powered systems are used for supply chain optim...\n",
      "Evaluating proposition 151/873...\n",
      "Evaluating proposition 151/873...\n",
      "Proposition failed quality check: AI is used in manufacturing for robotics....\n",
      "Proposition failed quality check: AI is used in manufacturing for robotics....\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     14\u001b[39m reference_answers = [\n\u001b[32m     15\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe main ethical concerns in AI development include bias and fairness, privacy, transparency, accountability, safety, and the potential for misuse or harmful applications.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# \"Explainable AI improves trust by making AI decision-making processes transparent and understandable to users, helping them verify fairness, identify potential biases, and better understand AI limitations.\",\u001b[39;00m\n\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m# \"Key challenges in developing fair AI systems include addressing data bias, ensuring diverse representation in training data, creating transparent algorithms, defining fairness across different contexts, and balancing competing fairness criteria.\",\u001b[39;00m\n\u001b[32m     18\u001b[39m     \u001b[38;5;66;03m# \"Human oversight plays a critical role in AI safety by monitoring system behavior, verifying outputs, intervening when necessary, setting ethical boundaries, and ensuring AI systems remain aligned with human values and intentions throughout their operation.\"\u001b[39;00m\n\u001b[32m     19\u001b[39m ]\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Run the evaluation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m evaluation_results = \u001b[43mrun_proposition_chunking_evaluation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_queries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_queries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreference_answers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreference_answers\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Print the overall analysis\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Overall Analysis ===\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mrun_proposition_chunking_evaluation\u001b[39m\u001b[34m(pdf_path, test_queries, reference_answers)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== Starting Proposition Chunking Evaluation ===\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Process document into propositions and chunks\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m chunks, propositions = \u001b[43mprocess_document_into_propositions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Build vector stores for chunks and propositions\u001b[39;00m\n\u001b[32m     19\u001b[39m chunk_store, prop_store = build_vector_stores(chunks, propositions)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mprocess_document_into_propositions\u001b[39m\u001b[34m(pdf_path, chunk_size, chunk_overlap, quality_thresholds)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEvaluating proposition \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_propositions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# Evaluate the quality of the current proposition\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m scores = \u001b[43mevaluate_proposition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprop\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprop\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msource_text\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m prop[\u001b[33m\"\u001b[39m\u001b[33mquality_scores\u001b[39m\u001b[33m\"\u001b[39m] = scores\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# Check if the proposition passes the quality thresholds\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mevaluate_proposition\u001b[39m\u001b[34m(proposition, original_text)\u001b[39m\n\u001b[32m     26\u001b[39m user_prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33mProposition: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproposition\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     27\u001b[39m \n\u001b[32m     28\u001b[39m \u001b[33mOriginal Text: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_text\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     29\u001b[39m \n\u001b[32m     30\u001b[39m \u001b[33mPlease provide your evaluation scores in JSON format.\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Generate response from the model using Azure OpenAI\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchat_deployment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_prompt\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjson_object\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\n\u001b[32m     41\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Parse the JSON response\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/RAG/all-rag-techniques/venv/lib/python3.13/site-packages/openai/_utils/_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/RAG/all-rag-techniques/venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py:1192\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1145\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1147\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1189\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1190\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1191\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1231\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1242\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/RAG/all-rag-techniques/venv/lib/python3.13/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/RAG/all-rag-techniques/venv/lib/python3.13/site-packages/openai/_base_client.py:982\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    980\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    988\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/RAG/all-rag-techniques/venv/lib/python3.13/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/RAG/all-rag-techniques/venv/lib/python3.13/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/RAG/all-rag-techniques/venv/lib/python3.13/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/RAG/all-rag-techniques/venv/lib/python3.13/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/RAG/all-rag-techniques/venv/lib/python3.13/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/RAG/all-rag-techniques/venv/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/RAG/all-rag-techniques/venv/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/RAG/all-rag-techniques/venv/lib/python3.13/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/RAG/all-rag-techniques/venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/RAG/all-rag-techniques/venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/RAG/all-rag-techniques/venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/RAG/all-rag-techniques/venv/lib/python3.13/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/RAG/all-rag-techniques/venv/lib/python3.13/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/ssl.py:1285\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1282\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1283\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1284\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1287\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/ssl.py:1140\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1142\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Path to the AI information document that will be processed\n",
    "pdf_path = \"data/AI_Information.pdf\"\n",
    "\n",
    "# Define test queries covering different aspects of AI to evaluate proposition chunking\n",
    "test_queries = [\n",
    "    \"What are the main ethical concerns in AI development?\",\n",
    "    # \"How does explainable AI improve trust in AI systems?\",\n",
    "    # \"What are the key challenges in developing fair AI systems?\",\n",
    "    # \"What role does human oversight play in AI safety?\"\n",
    "]\n",
    "\n",
    "# Reference answers for more thorough evaluation and comparison of results\n",
    "# These provide a ground truth to measure the quality of generated responses\n",
    "reference_answers = [\n",
    "    \"The main ethical concerns in AI development include bias and fairness, privacy, transparency, accountability, safety, and the potential for misuse or harmful applications.\",\n",
    "    # \"Explainable AI improves trust by making AI decision-making processes transparent and understandable to users, helping them verify fairness, identify potential biases, and better understand AI limitations.\",\n",
    "    # \"Key challenges in developing fair AI systems include addressing data bias, ensuring diverse representation in training data, creating transparent algorithms, defining fairness across different contexts, and balancing competing fairness criteria.\",\n",
    "    # \"Human oversight plays a critical role in AI safety by monitoring system behavior, verifying outputs, intervening when necessary, setting ethical boundaries, and ensuring AI systems remain aligned with human values and intentions throughout their operation.\"\n",
    "]\n",
    "\n",
    "# Run the evaluation\n",
    "evaluation_results = run_proposition_chunking_evaluation(\n",
    "    pdf_path=pdf_path,\n",
    "    test_queries=test_queries,\n",
    "    reference_answers=reference_answers\n",
    ")\n",
    "\n",
    "# Print the overall analysis\n",
    "print(\"\\n\\n=== Overall Analysis ===\")\n",
    "print(evaluation_results[\"overall_analysis\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
